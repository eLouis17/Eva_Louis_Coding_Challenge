# -*- coding: utf-8 -*-
"""DataScience_App.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MxyebsR_REhsi7kvpw2n1bN6goHEh0NU
"""

#Importing libraries
from sklearn import preprocessing
import numpy as np
import pandas as pd
from sklearn import datasets
from sklearn import metrics

#installing ucimlrepo to get the dataset

!pip install ucimlrepo

#The dataset containing demographic data
from ucimlrepo import fetch_ucirepo

# fetch dataset
adult = fetch_ucirepo(id=2)

# data (as pandas dataframes)
X = adult.data.features
Y = adult.data.targets

# metadata
print(adult.metadata)

# variable information
print(adult.variables)

#Checking the data entries in the X and Y dataframes to make sure there aren't duplicate entries with slightly different syntax and for missing values
for column in X:
    print(X[column].unique())
for column in Y:
    print(Y[column].unique())

#Standardzing all of the missing values to np.nan
X.replace(to_replace = "?", value = np.nan, inplace = True)

#Fixing the formatting error to remove the "." from some entries
Y.replace(to_replace = "<=50K.", value = "<=50K", inplace = True)
Y.replace(to_replace = ">50K.", value = ">50K", inplace = True)

#Saving the data as a csv
X.to_csv('x_data.csv')
Y.to_csv('y_data.csv')

#Looking at the data to see if one value is underrepresented
print(Y['income'].value_counts())

#We will keep in mind as we make our model that high income is underrepresented which may lead to biased results

#Changing String data to numerical data for use in the Neural Network
le = preprocessing.LabelEncoder()
income = le.fit_transform(list(Y["income"]))
sex = le.fit_transform(list(X["sex"]))
age = list(X["age"])
workclass = le.fit_transform(list(X["workclass"]))
fnlwgt = list(X["fnlwgt"])
education = le.fit_transform(list(X["education"]))
education_num = list(X["education-num"])
marital_status = le.fit_transform(list(X["marital-status"]))
occupation = le.fit_transform(list(X["occupation"]))
relationship = le.fit_transform(list(X["relationship"]))
race = le.fit_transform(list(X["race"]))
capital_gain = list(X["capital-gain"])
capital_loss = list(X["capital-loss"])
hours_per_week = list(X["hours-per-week"])
native_country = le.fit_transform(list(X["native-country"]))

#Combining each individual data column into one big list
x = list(zip(sex, age, workclass, fnlwgt, education, education_num, marital_status, occupation, relationship, race, capital_gain, capital_loss, hours_per_week, native_country))  # features
y = list(income)

#Importing various libraries
from sklearn.neural_network import MLPClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn import model_selection
from sklearn.metrics import accuracy_score
from collections import Counter
import tensorflow.keras as keras
from keras.models import Sequential
from keras.layers import Dense, Conv2D
from keras.layers import Activation, MaxPooling2D, Dropout, Flatten, Reshape
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import cross_val_score
from keras.losses import categorical_crossentropy
from keras.optimizers import SGD
from keras.callbacks import ModelCheckpoint
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
from tensorflow.keras.utils import to_categorical

#A function to later graph a confusion matrix of the results
label_map = {"0":"Low Income","1":"High Income"}
def plot_confusion_matrix(y_true,y_predicted):
  cm = metrics.confusion_matrix(y_true, y_predicted)
  print ("Plotting the Confusion Matrix")
  labels = list(label_map.values())
  df_cm = pd.DataFrame(cm,index = labels,columns = labels)
  fig = plt.figure()
  res = sns.heatmap(df_cm, annot=True,cmap='Blues', fmt='g')
  plt.yticks([0.5,1.5], labels,va='center')
  plt.title('Confusion Matrix - TestData')
  plt.ylabel('True label')
  plt.xlabel('Predicted label')

  plt.show()
  plt.close()

#Splitting the data to training and testing data making sure to leave 20% for testing
x_train, x_test, y_train, y_test = model_selection.train_test_split(x, y, test_size=0.2, random_state=1)

#Defines a neural network with 3 hidden layers with 5 neurons per layer and trains the model
nnet = MLPClassifier(hidden_layer_sizes=(5,5,5), random_state=1, max_iter=100000)
nnet.fit(x_train, y_train)

# Predict what the classes are based on the testing data
predictions = nnet.predict(x_test)

# Print the score on the testing data
print("MLP Testing Accuracy:")
print(accuracy_score(y_test, predictions)*100)

plot_confusion_matrix(y_test, predictions)

#I have defined Positive as being high income and Negative as being low income
#The false positive rate is equal to (# of false positives)/(# of false positives + # of true negatives)all positives and it is the opposite for the false negative rate

print("False Positive Rate: .0269%")
print("False Negative Rate: 96.88%")

#This model seems to clearly have bias against high income earners because the model ends up predicting most of the data to be a low income earner
#Because so much of the data has low-income earners, the can have a high accuracy just by assuming most people are low income earners.

# Let's try a k nearest neighbors model to try and eliminate some bias
knn = KNeighborsClassifier(n_neighbors = 13, weights = 'distance')

# train model with X_train
knn.fit(x_train, y_train)

# predict testing data
y_predict = knn.predict(x_test)

# print accuracy score
print("KNN Testing Accuracy:")
print(accuracy_score(y_test, y_predict)*100)

plot_confusion_matrix(y_test, y_predict)

print("False Positive Rate: 4.98%")
print("False Negative Rate: 69.17%")
#The accuracy rate for this model slightly improved and the false negative rate decreased slightly.